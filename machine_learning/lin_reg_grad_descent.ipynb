{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred=array([[2.],\n",
      "       [3.],\n",
      "       [4.]])\n",
      "loss=np.float64(0.16666666666666666)\n",
      "loss_grad=array([[0.66666667],\n",
      "       [0.66666667],\n",
      "       [0.66666667]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.98, 0.96]), array([-0.02]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    # Your code here, make sure to round\n",
    "    m, n = X.shape  # (3, 2)\n",
    "    w = np.ones((n, 1))\n",
    "    y = y.reshape(y.shape[0], 1)\n",
    "    # bias has shape (output_num,)\n",
    "    b = np.zeros((1,))\n",
    "    for i in range(1):\n",
    "        # -- forward pass --\n",
    "        y_pred = X @ w + b\n",
    "        print(f'{y_pred=}')\n",
    "        # MSE\n",
    "        loss = np.mean((y_pred - y) ** 2)/(2*m)\n",
    "        print(f'{loss=}')\n",
    "\n",
    "        # -- backward pass --\n",
    "        loss_grad = 2*(y_pred-y)/m\n",
    "        print(f'{loss_grad=}')\n",
    "        # Transpose so that the multiplication is (2, 3) * (1, 3), so we can multiply on the columns\n",
    "        w_grad = X.T @ loss_grad\n",
    "        # As the derivative of b respect to the loss is 1, it updates over all gradients equally, so we need the sum of loss grad\n",
    "        b_grad = np.sum(loss_grad)\n",
    "\n",
    "        # -- update --\n",
    "        w -= alpha * w_grad\n",
    "        b -= alpha * b_grad\n",
    "\n",
    "    return np.round(w.flatten(), 4), b\n",
    "\n",
    "linear_regression_gradient_descent(X = np.array([[1, 1], [1, 2], [1, 3]]), y = np.array([1, 2, 3]), alpha = 0.01, iterations = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mis understood the problem, I will be combining w and b as a single theta value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11071521],\n",
       "       [0.95129619]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "\t# Your code here, make sure to round\n",
    "\tm, n = X.shape\n",
    "\ttheta = np.zeros((n, 1))\n",
    "\ty = y.reshape(m, 1)\n",
    "\tfor _ in range(iterations):\n",
    "\t\ty_pred = X @ theta\n",
    "\t\t# Apparently this solution uses MSE without 1/2m, hence the 2 is not there\n",
    "\t\tloss_grad = (y_pred-y) / m\n",
    "\t\ttheta_grad = X.T @ loss_grad\n",
    "\t\ttheta -= alpha * theta_grad\n",
    "\treturn theta\n",
    "\n",
    "linear_regression_gradient_descent(X = np.array([[1, 1], [1, 2], [1, 3]]), y = np.array([1, 2, 3]), alpha = 0.01, iterations = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
